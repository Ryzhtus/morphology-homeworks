{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Working copy of SOTA POS-TAG without dataloader.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python38064bit342b2d62f7794dc18ccc35f134af38b3",
   "language": "python",
   "display_name": "Python 3.8.0 64-bit"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "LgRn9eFYlrE9"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "InZmKhUvqynt"
   },
   "source": [
    "class Sentence():\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.pos_tags = []\n",
    "        self.grams = []"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z6A3salirMZ8"
   },
   "source": [
    "def read_dataset(dataset, mode):\n",
    "    sentences = []\n",
    "    \n",
    "    with open(dataset, mode='r', encoding='utf-8') as data:\n",
    "        # Пропускаем заголовок\n",
    "        next(data)\n",
    "        \n",
    "        sentence = Sentence() # будем заполнять список предложений\n",
    "        \n",
    "        for row in data:\n",
    "            row = row.strip()\n",
    "            if len(row) != 0:\n",
    "                row = row.split('\\t')\n",
    "\n",
    "                if mode == 'train':\n",
    "                    _, _, token, pos_gram = row \n",
    "                    pos, gram = pos_gram.split('#')\n",
    "\n",
    "                else:\n",
    "                    _, _, token = row\n",
    "                    pos, gram = '<UNK>', '<UNK>'\n",
    "\n",
    "                sentence.tokens.append(token)\n",
    "                sentence.pos_tags.append(pos)\n",
    "                sentence.grams.append(gram)\n",
    "\n",
    "            else:\n",
    "                if len(sentence.tokens) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = Sentence()\n",
    "                \n",
    "        if len(sentence.tokens) > 0:\n",
    "            sentence.append(sentence)\n",
    "            \n",
    "    return sentences"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3uaccMRJrN8q"
   },
   "source": [
    "train = read_dataset('data/train.csv', 'train')\n",
    "test = read_dataset('data/test.csv', 'test')"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oEoAYzMbrQEE"
   },
   "source": [
    "def get_vocabulary(data):\n",
    "    vocabulary = set()\n",
    "    for sentence in data:\n",
    "        for token in sentence.tokens:\n",
    "            vocabulary.add(token)\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "def get_tags(data):\n",
    "    tags = set()\n",
    "    for sentence in data:\n",
    "        for tag in sentence.pos_tags:\n",
    "            tags.add(tag)\n",
    "\n",
    "    return tags\n",
    "\n",
    "def get_chars(data):\n",
    "    chars = set()\n",
    "    for sentence in data:\n",
    "        for token in sentence.tokens:\n",
    "            for char in token:\n",
    "                chars.add(char)\n",
    "\n",
    "    return chars"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nfSrSBixqPb3"
   },
   "source": [
    "#запомним все уникальные слова и POS-теги в train \n",
    "train_vocab = get_vocabulary(train)\n",
    "train_tags = get_tags(train)\n",
    "train_chars = get_chars(train)\n",
    "\n",
    "train_token_index = {word: i for i, word in enumerate(train_vocab)}\n",
    "train_tag_index = {tag: i for i, tag in enumerate(train_tags)}\n",
    "train_char_index = {char: i for i, char in enumerate(train_chars)}\n",
    "\n",
    "#запомним все уникальные слова и POS-теги в test\n",
    "test_vocab = get_vocabulary(test)\n",
    "test_tags = get_tags(test)\n",
    "test_chars = get_chars(test)\n",
    "\n",
    "test_token_index = {word: i for i, word in enumerate(test_vocab)}\n",
    "test_tag_index = {tag: i for i, tag in enumerate(test_tags)}\n",
    "test_char_index = {char: i for i, char in enumerate(test_chars)}"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y-qBhmEbsQ4f"
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, device, token_indexer, tag_indexer, char_indexer):\n",
    "        self.sentences = sentences\n",
    "        self.device = device\n",
    "        self.token_indexer = token_indexer\n",
    "        self.tag_indexer = tag_indexer\n",
    "        self.char_indexer = char_indexer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.get_tensor_from_token(self.sentences[item]), self.get_tensor_from_tag(self.sentences[item]), self.get_tensor_from_char(self.sentences[item])\n",
    "\n",
    "    def get_tensor_from_token(self, sentence):\n",
    "        indicies = [self.token_indexer.get(token) for token in sentence.tokens]\n",
    "        \n",
    "        return torch.LongTensor(indicies)\n",
    "\n",
    "    def get_tensor_from_tag(self, sentence):\n",
    "        indicies = [self.tag_indexer.get(tag) for tag in sentence.pos_tags]\n",
    "        \n",
    "        return torch.LongTensor(indicies)\n",
    "\n",
    "    def get_tensor_from_char(self, sentence):\n",
    "        indicies = []\n",
    "        char_indicies = []\n",
    "        for token in sentence.tokens:\n",
    "            for char in token:\n",
    "                char_indicies.append(self.char_indexer.get(char))\n",
    "            indicies.append(torch.LongTensor(char_indicies).to(device))\n",
    "            char_indicies = []\n",
    "\n",
    "        return indicies"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XsYfzg5UvaD5"
   },
   "source": [
    "dev = train[30000: 40000]\n",
    "train = train[:30000]"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dDX_DhpOvc40"
   },
   "source": [
    "TrainDataset = CustomDataset(train, device, train_token_index, train_tag_index, train_char_index)\n",
    "DevDataset = CustomDataset(dev, device, train_token_index, train_tag_index, train_char_index)\n",
    "TestDataset = CustomDataset(test, device, test_token_index, test_tag_index, test_char_index)"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lpuLFejLvmKM"
   },
   "source": [
    "class DualLSTMTagger(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, word_hidden_dim, char_embedding_dim, char_hidden_dim, word_vocab_size, char_vocab_size, tag_vocab_size):\n",
    "        super(DualLSTMTagger, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(word_embedding_dim + char_hidden_dim, word_hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(word_hidden_dim, tag_vocab_size)\n",
    "        \n",
    "    def forward(self, sentence, words):\n",
    "        embeds = self.word_embedding(sentence)\n",
    "        char_hidden_final = []\n",
    "        for word in words:\n",
    "            char_embeds = self.char_embedding(word)\n",
    "            _, (char_hidden, char_cell_state) = self.char_lstm(char_embeds.view(len(word), 1, -1))\n",
    "            word_char_hidden_state = char_hidden.view(-1)\n",
    "            char_hidden_final.append(word_char_hidden_state)\n",
    "        char_hidden_final = torch.stack(tuple(char_hidden_final))\n",
    "        \n",
    "        combined = torch.cat((embeds, char_hidden_final), 1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(combined.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        \n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zJskg--IOL9j"
   },
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sAgbRNZSyNI9"
   },
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, train_iterator, dev_iterator, lr=2e-5, device=device):\n",
    "        self.criterion = nn.CrossEntropyLoss().to(device)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.train_iterator = train_iterator\n",
    "        self.dev_iterator = dev_iterator\n",
    "        self.model = model.to(device)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for batch_idx, (tokens, pos_tags, chars) in tqdm(enumerate(self.train_iterator), total=len(self.train_iterator)):\n",
    "            self.optimizer.zero_grad()\n",
    "            tokens = tokens.to(device)\n",
    "            #print(tokens)\n",
    "            #print(chars)\n",
    "            chars = [char.to(device) for char in chars]\n",
    "            pos_tags = pos_tags.to(device)\n",
    "            #print()\n",
    "            #print('Tokens shape {}'.format(tokens.shape))\n",
    "            #print('Tags shape {}'.format(pos_tags.shape))\n",
    "            #print('Chars shape {}'.format(chars.shape))\n",
    "            logits = self.model(tokens, chars)\n",
    "            #print(f'logits.view(-1, logits.size(-1)).shape: {logits.view(-1, logits.size(-1)).shape}')\n",
    "            #print(f'pos_tags.view(-1).shape: {pos_tags.view(-1).shape}')\n",
    "            loss = self.criterion(logits.view(-1, logits.size(-1)), pos_tags.view(-1))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            mask = (tokens != 0).to(torch.long)\n",
    "            pred = torch.argmax(logits, dim=-1)\n",
    "            correct += ((pred == pos_tags)*mask).sum().item()\n",
    "            total += mask.sum().item()\n",
    "\n",
    "        print('\\rLoss: %4f, Accuracy: %4f, Batch: %d of %d' % (\n",
    "            total_loss / (batch_idx + 1), correct / total, batch_idx + 1, len(self.train_iterator)\n",
    "        ), end='')\n",
    "        print()\n",
    "\n",
    "    def test_epoch(self):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for batch_idx, (tokens, pos_tags, chars) in enumerate(self.dev_iterator):\n",
    "                logits = self.model(tokens, chars)\n",
    "                loss = self.criterion(logits.view(-1, logits.size(-1)), pos_tags.view(-1))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                mask = (tokens != 0).to(torch.long)\n",
    "                pred = torch.argmax(logits, dim=-1)\n",
    "                correct += ((pred == pos_tags) * mask).sum().item()\n",
    "                total += mask.sum().item()\n",
    "\n",
    "            print('\\rLoss: %4f, Accuracy: %4f, Batch: %d of %d' % (\n",
    "                total_loss / (batch_idx + 1), correct / total, batch_idx + 1, len(self.dev_iterator)\n",
    "            ), end='')\n",
    "            print()"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O0DGStt7yVcz"
   },
   "source": [
    "WORD_EMBEDDING_DIM = 1024\n",
    "CHAR_EMBEDDING_DIM = 128\n",
    "WORD_HIDDEN_DIM = 1024\n",
    "CHAR_HIDDEN_DIM = 1024\n",
    "EPOCHS = 10\n",
    "\n",
    "model = DualLSTMTagger(WORD_EMBEDDING_DIM, WORD_HIDDEN_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, len(train_vocab), len(train_chars), len(train_tags))\n",
    "trainer = Trainer(model, TrainDataset, DevDataset)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6Hj5_IcZy0Ni",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "134e37bf-6a39-489c-9700-679e98e8c13c"
   },
   "source": [
    "for epoch in tqdm(range(10)):\r\n",
    "    print()\r\n",
    "    trainer.train_epoch()\r\n",
    "    trainer.test_epoch()"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/30000 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "error in LoadLibraryA",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-15-7373690bad0d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m     \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtest_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-13-a6b3f8d85afa>\u001B[0m in \u001B[0;36mtrain_epoch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     23\u001B[0m             \u001B[1;31m#print('Tags shape {}'.format(pos_tags.shape))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m             \u001B[1;31m#print('Chars shape {}'.format(chars.shape))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m             \u001B[0mlogits\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mchars\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m             \u001B[1;31m#print(f'logits.view(-1, logits.size(-1)).shape: {logits.view(-1, logits.size(-1)).shape}')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m             \u001B[1;31m#print(f'pos_tags.view(-1).shape: {pos_tags.view(-1).shape}')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    531\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 532\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    533\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-adbd9310a0c3>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, sentence, words)\u001B[0m\n\u001B[0;32m     18\u001B[0m             \u001B[0mword_char_hidden_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mchar_hidden\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m             \u001B[0mchar_hidden_final\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword_char_hidden_state\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m         \u001B[0mchar_hidden_final\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mchar_hidden_final\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m         \u001B[0mcombined\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membeds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mchar_hidden_final\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: error in LoadLibraryA"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7rXyCdYgy2gI"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}